{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30261ca6-e93c-46e8-8efc-fce9e8cd9f85",
   "metadata": {},
   "source": [
    "##Download and prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d977847-cd34-436a-b5b8-4d04ecddfe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened tarfile\n",
      "All files extracted\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#split data into training, validation and testing sets\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './train'"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import requests, zipfile, sys\n",
    "import tarfile\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Download train.tar -file from miniImageNet datset\n",
    "miniimagenet_path = \"./mini-imagenet/train.tar\n",
    "with tarfile.open(miniimagenet_path, \"r\") as tf:\n",
    "    print(\"Opened tarfile\")\n",
    "    tf.extractall(path=\"./\")\n",
    "    print(\"All files extracted\")\n",
    "\n",
    "data_dir = \"./train\"\n",
    "data = np.load(data_dir)\n",
    "\n",
    "\n",
    "#download EuroSAT(RGB) dataset\n",
    "eurosat_path = \"./EuroSAT_RGB\"\n",
    "try: \n",
    "    contents = os.listdir(folder_path)\n",
    "    print(\"Contents of the folder:\", contents)\n",
    "except FileNotFoundError:\n",
    "    print(f\"The folder at {folder_path} was not found.\")\n",
    "\n",
    "\n",
    "#split data into training, validation and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1104e48-de1b-4e1f-92c6-d2c000fcdf34",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2117c-4b32-4f61-a273-d5f4e1730de4",
   "metadata": {},
   "source": [
    "## Pretrain a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8245980a-ccfc-4e32-bc3b-f1097d3f7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "# Image Size\n",
    "image_size = 32\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 30\n",
    "\n",
    "#Dataset to tensor\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root='./train.............',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    ")\n",
    "#Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.axis('off')\n",
    "plt.title('The Training Dataset')\n",
    "plt.imshow(np.transpose(utils.make_grid(real_batch[0].to(device)[:36], padding=2, normalize=True, nrow=6).cpu(),(1,2,0)))\n",
    "\n",
    "#model selection\n",
    "\n",
    "#training\n",
    "\n",
    "#evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320947fc-372a-44bd-89d8-c6a465f514b5",
   "metadata": {},
   "source": [
    "##Fine-tuning with EuroSAT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2f9d3-7142-40ee-a451-66c5bd9c04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data\n",
    "\n",
    "#fine tune\n",
    "\n",
    "#tesing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1cfa4-6ba4-44c6-8a55-44a9f09ed75a",
   "metadata": {},
   "source": [
    "##Model comparison and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047f443-0903-4bf6-817b-61920a3e8765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76c5ccc0-8900-4faa-88bd-5ebb2e097910",
   "metadata": {},
   "source": [
    "##Evaluation on additional dataset (bonus tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd436ee-602f-4974-8590-de1de16f7030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be09c85-fc5b-42dd-927a-e64f5ab3ec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
